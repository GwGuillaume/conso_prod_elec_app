# api_client.py
# -*- coding: utf-8 -*-
"""
Client pour l'API Conso (consommation √©lectrique).

Fonctions principales :
- V√©rifie si un JSON pour une date est d√©j√† dans l'archive
- T√©l√©charge les donn√©es 1h ou 30min si manquantes
- Concat√®ne les CSV
- Archive les JSON et supprime les fichiers locaux

Utilise :
- config.py pour token, PRM et chemins de fichiers
- common/utils.py pour la gestion des fichiers et archives
"""

import pandas as pd
from datetime import datetime, timedelta
import requests
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv
from os import getenv

from conso_api_tools import config
from common.utils import ensure_folder, add_file_to_zip, save_json, check_json_in_archive, format_date_to_str, next_day, format_str_to_date

# -------------------------------
# ‚öôÔ∏è CONFIGURATION DES DOSSIERS
# -------------------------------

ensure_folder(config.FOLDER_1H)
ensure_folder(config.FOLDER_30MIN)

# -------------------------------
# üîß FONCTIONS PRINCIPALES
# -------------------------------

def _current_token() -> Optional[str]:
    """Retourne le token ENEDIS_TOKEN depuis .env ou l'environnement."""
    load_dotenv(override=True)  # recharge le .env √† chaque appel
    return getenv("ENEDIS_TOKEN")

def _get_headers(token: Optional[str] = None) -> dict:
    """
    Construit les en-t√™tes pour les requ√™tes Enedis.
    """
    tok = token or _current_token()
    if not tok:
        raise RuntimeError("‚ö†Ô∏è Aucun token disponible (ENEDIS_TOKEN non d√©fini).")
    return {
        "Authorization": f"Bearer {config.ENEDIS_TOKEN}",
        "Accept": "application/json",
        "User-Agent": "github.com/gguillaume/conso-prod-elec",
        "From": "contact@example.com",
    }

def download_interval_data(date_str: str, interval: str = "30min") -> dict | None:
    """
    T√©l√©charge les donn√©es de consommation pour une date donn√©e et un intervalle.

    Param√®tres :
        date_str (str) : date au format 'YYYY-MM-DD'
        interval (str) : '30min' ou '1h'

    Retour :
        dict : donn√©es JSON renvoy√©es par l'API
    """
    day_after = format_date_to_str(next_day(format_str_to_date(date_str)))
    url = f"{config.API_BASE_URL}?prm={config.LINKY_PRM}&start={date_str}&end={day_after}"

    response = requests.get(url, headers=_get_headers(), timeout=20)
    try:
        response.raise_for_status()
        data = response.json()
        if "interval_reading" not in data:
            raise ValueError(f"Aucune donn√©e disponible pour {date_str} ({interval})")
        return data
    except requests.exceptions.HTTPError as err:
        err_mess = err.args[0]
        end_date_str = err_mess[err_mess.rfind('&end=')+5:]
        end_date = format_str_to_date(end_date_str).date()
        if end_date == datetime.today().date():
            print(f"‚ùå Donn√©es du {end_date} pas encore disponibles, pas de t√©l√©chargement.")
            pass
        else:
            raise SystemExit(err)

def append_to_csv(data: dict, csv_file: Path):
    """
    Concat√®ne les valeurs JSON dans le CSV correspondant.

    Param√®tres :
    ------------
        data (dict) : Donn√©es JSON de la courbe de charge.
        csv_file (Path) : Chemin du fichier CSV √† mettre √† jour.

    D√©tails :
    ---------
        - Utilise le s√©parateur ';' pour rester coh√©rent avec les fichiers de production.
        - Ajoute les donn√©es sans r√©√©crire l'en-t√™te si le fichier existe d√©j√†.
        - Force l'encodage UTF-8 avec BOM pour compatibilit√© Windows.
    """
    rows = []
    # unit = data.get("reading_type", {}).get("unit", "")
    for item in data.get("interval_reading", []):
        rows.append({
            "datetime": item["date"],
            f"consommation": item["value"]
        })
    df = pd.DataFrame(rows)
    if csv_file.exists():
        df.to_csv(
            csv_file,
            mode="a",
            header=False,
            index=False,
            sep=";",
            encoding="utf-8-sig")
    else:
        df.to_csv(
            csv_file,
            index=False,
            sep=";",
            encoding="utf-8-sig")

def fetch_and_archive(date_obj: datetime, interval: str):
    """
    T√©l√©charge les donn√©es si elles ne sont pas d√©j√† archiv√©es,
    les ajoute au CSV et au ZIP, puis supprime le JSON local.

    Param√®tres :
        date_obj (datetime) : date cibl√©e
        interval (str) : '30min' ou '1h'

    Retour :
        bool : True si une requ√™te API a √©t√© effectu√©e, False sinon
    """
    date_str = format_date_to_str(date_obj)
    folder = config.FOLDER_30MIN if interval == "30min" else config.FOLDER_1H
    csv_file = config.CSV_30MIN if interval == "30min" else config.CSV_1H
    interval_folder_name = "conso_30min" if interval == "30min" else "conso_1h"

    if check_json_in_archive(zip_path=config.ZIP_FILE,
                             date_str=date_str,
                             interval_folder=interval_folder_name):
        print(f"üìÇ {interval_folder_name}/courbe_{date_str}.json d√©j√† pr√©sent dans l‚Äôarchive, pas de t√©l√©chargement.")
        return False

    # T√©l√©chargement
    data = download_interval_data(date_str, interval)
    if data is None:
        return False
    else:
        json_path = save_json(data, date_str, folder)
        # Concat√©nation du CSV
        append_to_csv(data, csv_file)
        print(f"üßæ Donn√©es ajout√©es √† {csv_file}")

        # Archive et suppression
        arcname = f"{interval_folder_name}/courbe_{date_str}.json"
        add_file_to_zip(
            tmp_file=json_path,
            zip_path=config.ZIP_FILE,
            arcname=arcname
        )
        # add_file_to_zip(json_path, config.ZIP_FILE, cname=f"{interval_folder_name}/courbe_{date_str}.json")
        json_path.unlink()
        print(f"‚úÖ {json_path.name} archiv√© et supprim√© localement")

        return True
